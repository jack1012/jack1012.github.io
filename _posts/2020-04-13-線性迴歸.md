---
layout      : post
title       : 線性迴歸
date        : 2020-04-10 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,線性迴歸,多元線性迴歸]
---

# 線性迴歸
$$y = \hat{y}+\epsilon = f(x_1,x_2,\cdots,x_m) + \epsilon$$
當 $y$ 是數值且 $f(\cdot)$ 為線性時，我們要建立線性迴歸模型。
PS：線性是指  $f$ 函數的參數是線性的，$x$ 可以不是線性的，例如取對數。

模型參數估計的方法普通最小平方法(Ordinary Least Squares, OLS) 是以誤差平方和(Sum Square Error, SSE)估計迴歸方程式的各項係數，或殘差平方和(Residual Sum of Square, RSS)為最小化目標。R語言中的 \{stats\}套件中 `lm()` 函數。

## 簡單線性迴歸
$$ y_i=\hat{y}_i+\epsilon_i = b_0 + b_1 x_{i} +\epsilon_i$$

$y_i$：第 $i$ 個變數的反應變數真實值
$\hat{y}_i$：第 $i$ 個變數的反應變數預測值
$\epsilon_i$：樣本殘差(或誤差)項，也就是模型無法解釋的隨機誤差
$b_0$：截距估計值
$b_1$：迴歸係數
$x_i$：第 $i$ 個樣本的預測變數

## 多元線性迴歸
$$ y_i =\hat{y}_i+\epsilon_i=b_0+b_1x_{i1}+b_2x_{i2}+ \cdots+b_mx_{im}+\epsilon_i $$

$y_i$ ：第 $i$ 個變數的反應變數真實值
$\hat{y}_i$ ：第 $i$ 個變數的反應變數預測值
$\epsilon_i$：是樣本殘差(或誤差)項，也就是模型無法解釋的隨機誤差
$b_0$：截距估計值
$b_1 \cdots b_m$：迴歸係數
$x_{ij}$：第 $i$ 個樣本其第 $j$ 個的預測變數值

## 進階討論
1. 關於求解線性迴歸 $Y= β^{T} X = β_0 \times  1+ β_1 x_1 + β _2 x_2 + \cdots β_m x_m$ 其中 $β^{T} = [ β_0, β_1 , \cdots , β_m ]$
- 簡單線性迴歸，只包括一個自變量和一個因變量，可使用最小平方法求解
- 簡單線性迴歸解得模型是否有效，需要計算相關係數 $r$ 以確認 $X$ 對 $Y$ 有顯著的影響，並呈密切的線性相關
- 多元線性迴歸自變數之間的相關程度"不"可以高於自變數與因變數之間的相關程度
- 多元迴歸公式的結構等於一個只有輸入和輸出層的神經網路，可用隨機梯度下降法 (Stochastic gradient descent, SGD)去找 $β^{T}$ 的最佳解

2. 迴歸分析的基本統計假設：
- 依變數和自變數之間的關係必須是線性
- 資料呈現常態分配（Normal Distribution）
- 自變數的誤差項，相互之間應該是獨立的

3. 在線性迴歸中，以最小平方法計算迴歸參數時，殘差需符合四大條件。下列何者非屬四大條件之一？
- 殘差期望值為零
- (x) 殘差必須符合均勻分配 --> 常態分配
- 殘差之間沒有自相關（殘差獨立性）
- 殘差需符合變異數同質性

4. 若希望能透過歷史氣溫與菜價資料來預測未來菜價，運用以下何種工具較為適當？A
- (A) 線性迴歸模型
- (B) 分類模型
- (C) 集群分析
- (D) 探索式分析

5. 當所有觀察值都落在迴歸直線上，則x與y之間的相關係數為何？D
- (A) -1 < r < 1
- (B) 僅r = 1
- (C) 僅r = -1
- (D) r = 1 或 r = -1

6. 考慮簡單線性迴歸（Linear Regression）模型，其變異數分析表中，迴歸模型的自由度為何？ (A) 1

7. 資料分析師如欲估計線性迴歸模型的變異性，可以用重抽樣方法（Resampling Methods）對每一組重抽樣訓練樣本配適模型後，檢視各模型績效的差異程度，這種作法使我們可以獲得只以原訓練集配適一次因而無法獲得的有用資訊

8. 利用判定係數(coefficient of determination, $R^2$ )估計迴歸方程式之適合度(goodness of fit)，其必須先經過顯著性檢定(斜率 $\beta_1$ 是否等於 0 的假設檢定)。當未達顯著性水準時，判定係數 $R^2$ 數值高低不具任何意義；達到顯著性水準時，判定係數 $R^2$ 數值高低才具有代表性意義。

9. 各類分析方法
- 當自變項為「連續」，依變項為「連續」，可採用相關分析、簡單/多元迴歸分析、路徑分析等。
- 當自變項為「連續」，依變項為「不連續」，可採用羅吉斯分析、區別分析等。
- 當自變項為「不連續」，依變項為「連續」，可採用獨立/相依t檢定、變異數分析、共變數分析等。
- 當自變項為「不連續」，依變項為「不連續」，可採用描述性統計分析、卡方檢定等。
![自變項與依變項的測量尺度的關係圖](scale2-1.png)

10. 關於線性迴歸模型績效評估，下列敘述何者不正確？ B
- (A) 評估模型績效的方式不只一種
- (B) 通常可透過混淆矩陣評估模型績效 --> 殘差
- (C) 殘差繪圖（Residual Plots）是以視覺化的方式檢視模型的配適狀況
- (D) 許多績效評量的計算是基於殘差（Residual），它是各觀測值減去其模型的預測值，而常用的SSE是殘差平方的總和

11. 關於迴歸模型績效評估，下列敘述何者正確？D
- (A) 評估模型績效的方式不只一種，通常只用一種模型績效評估的方式來決定模型的優劣
- (B) 許多績效評量的計算是基於殘差（residual），它是模型的預測值減去觀測值
- (C) 常用的績效評量SSE是殘差平方的平均值，而另一個常用的績效評量MSE是殘差平方的總和
- (D) R2也是常用的績效評量，其值表示資料中的訊息被模型所解釋的比例，R2的解釋與結果變數的變異有關

12. 線性迴歸、偏最小平方法（PLS）、類神經網絡（NN）等算法內嵌有變數選擇機制的方法，對於預測變數中的雜訊，或是無訊息力的變數等較不敏感

13. 關於多元迴歸（Multiple Regression）與廣義線性模型（Generalized Linear Model，GLM），下列敘述何者不正確？ A
- (A) 多元迴歸的自變項必須是類別資料
- (B) GLM的自變項可以是類別資料
- (C) 多元迴歸的應變項必須為單一個
- (D) GLM的自變項可以是兩個以上

14. 關於線性迴歸，下列敘述何者不正確？D
- (A) 並非任何資料集均可建立多元線性迴歸模型（multiple linear regression），有時會有建模失敗的狀況發生
- (B) 線性迴歸屬於無母數（nonparametric）的統計建模方法 X
- (C) 迴歸方程式係數估計最佳化問題是最小化誤差平方和（Sum of Squared Error, SSE）
- (D) 相較於類神經網路與支援向量機等監督式學習模型，迴歸建模所獲得的模型可解釋性較低
