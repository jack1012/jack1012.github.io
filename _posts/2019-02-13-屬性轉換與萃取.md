---
layout      : post
title       : 屬性轉換與萃取
date        : 2019-02-01 10:41:10 +0800
categories  : 資料處理與分析概論
tags        : [資料處理,屬性轉換與萃取,屬性工程]
---

# 1-3.屬性轉換與萃取

屬性工程：屬性轉換和資料縮減
- 屬性轉換：除了各種變數的編碼之外，還包含透過公式降低變數尺度不統一，變數偏斜與離群資料對模型的不良引響，以提升模型績效
- 資料縮減：橫向的資料抽樣，縱向的屬性萃取與挑選，屬性萃取指結合多個預測變數成為一個代理變數或稱為潛在變數；屬性挑選則選擇移除訊息貧瘠、贅餘或無關的變數。


#### 練習題
215 資料特徵
A 215. 關於資料特徵，下列敘述何者不正確？
(A) 資料特徵個數越多，該模型所需的運算時間也就越短
(B) 資料特徵個數越多，容易引起維度災難，而模型也會越複雜
(C) 剔除不相關或多餘的資料特徵，以減少資料特徵個數，提高模型效果
(D) 可透過模型計算資料特徵重要程度，例如： Random Forest

41模型複雜度
C 41. 模型複雜度與預測誤差之間的變化關係，通常是越複雜的模型與訓練集合配適的越好。因此，一般而言訓練集的預測誤差，會隨著模型複雜度如何變化？
(A) 增加而增加
(B) 減少而減少
(C) 增加而減少
(D) 減少而增加

## 屬性轉換
### 屬性轉換目的

- 轉換後可能更容易發現資料之間的關係，使沒有關係變成有關係
- 資料可能呈現嚴重的偏態分布，經過轉換後差異可以拉開
- 讓資料能夠符合模型所需要的假設，以利進行分析，例如經過轉換後的資料呈現常態分布
- 除了各種變數的編碼之外，還包含透過公式降低變數尺度不統一，變數偏斜與離群資料對模型的不良引響，以提升模型績效

### 標準化、正規化
- 標準化(standardiztion)：將資料轉換後平均數為0、標準差為1
- 正規化(normalization)：將資料轉換後全距落在[0,1]或[-1,1]

### 分佈退化現象
分佈退化現象：分佈退化的預測變數對模型沒有貢獻
- 數值變數：可以觀察變異數是否為零變異，或接近零，亦可以由直方圖或密度曲線觀察是否偏態或單一高峰。
  - 計算變數最大值與最小值之比值檢視偏斜狀況
  - R語言{caret}套件的nearZeroVar()函數挑出變異數為零或幾乎為零的屬性
  - R語言{caret}套件的skewness()函數計算變數的偏態係數，通常門檻值為正負3

- 類別變數：可以觀察次數分佈表、長條圖、圓餅圖、percentUnique、freqRatio
    - percentUnique：獨一無二的類別數值數量與樣本大小的比集，如果太高代表類別變數的互異類別值非常多，宜刪除此類準識別欄位。如果等於1，代表每個樣本都是獨一無二，對模型沒有貢獻，宜刪除此類。
    - freqRatio：最頻繁的類別值頻次除以次頻繁類別變數值頻次的比值。如果太高代表類別變數高度集中在最頻繁的類別，宜刪除此高度偏斜變數。

- Box-Cox轉換：一種數據變換，用於連續變量不滿足常態分佈的情況。

參考閱讀：<https://www.itread01.com/content/1510815498.html>


### 常見的資料轉換
- 對數轉換：適合於正值資料，且變數值涵蓋兩個以上的量綱(10的冪次)
- 值域受限變數的變換
- 均等化變異的轉換
- 線性關係轉換
- log-log迴歸轉換
- 平滑化是用在資料中充滿大量的雜訊，讓資料看起來很亂的情況下。透過平滑化的方法，可以讓雜訊產生的干擾降低。
- 標準化是為了太過細節的資料，變得比較標準。
- 正規化則是要讓資料的數值縮減到一個區間內，避免讓某個屬性的影響被放大或縮小

- 連續變數離散化
- 離散變數連續化
- 日期型變數轉換(週期函數處理)

- 虛擬編碼（ Dummy Encoding）& OneHotEncoding

進階閱讀：<https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/463213/>

#### 練習題
213 屬性轉換目的
D 213. 下列何者不是屬性轉換的主要目的？
(A) 轉換後可能更容易發現資料之間的關係，使沒有關係變成有關係
(B) 資料可能呈現嚴重的偏態分布，經過轉換後差異可以拉開
(C) 讓資料能夠符合模型所需要的假設，以利進行分析，例如經過轉換後的資料呈現正態分布
(D) 能夠讓資料的可讀性更高



61數值標準化
A 61. 若一個樣本的平均數為 100，標準差為 10，擇一個數值為 200，則該數值標準化（ Standardarization）後，數值將會轉變為？
(A) 10
(B) 20
(C) 100

114 資料標準化
A 114. 關於資料標準化（ Normalization），下列敘述何者不正確？
(A) 通常會在建模後進行
(B) 消除不同觀測值間的差異性
(C) 減少不必要的變異
(D) 將不同變量縮放到一定範圍

115 標準化
B 115. 若您想對 30000 筆的房屋資料進行機器學習，欄位包含房價、樓層、距市區距離、車位數量和面積等，應該先對資料進行下列何項步驟，使得變量尺度統一？
(A) 集中化（ Centralization）
(B) 標準化（ Normalization）
(C) 聚合（ Aggregation）
(D) 離散化（ Discretization）


11 MinMax正規化
B 11. 有一群客戶的消費額最大為 3800 元、 最小為 1800 元。 假設將資料經過最小最大正規化（ Min-Max Normalization）轉換成 0 到 1 的範圍區間，則若一客戶的消費額為 2300 元時，該消費額會被轉換為什麼數字？
(A) 0.2
(B) 0.25
(C) 0.4
(D) 0.5

211資料特徵縮放
B 211. 對於某些資料屬性內出現異常大的值，有可能會導致誤導模型訓練的結果，此時會將該屬性值進行何種處理，使所有屬性值被轉換到 0 至 1 之間？
(A) 資料組織
(B) 資料特徵縮放
(C) 資料清理
(D) 資料分析

13特徵轉換
D 13. 下列何種方法可以用來進行特徵轉換？
(A) Diffusion maps
(B) Locally-linear embedding
(C) Relational perspective map
(D) 以上皆是

進階閱讀：
<https://kknews.cc/zh-tw/news/83q6exe.html>
<https://kknews.cc/finance/l8v5zm2.html>
<https://kknews.cc/game/462xn4x.html>

63裝箱法
B 63. 下列何種方法可以把學生的成績從連續型數值轉變為離散型的級距？
(A) 最大正規化（ Min-Max Normalization）
(B) 裝箱法（ Binning Method）
(C) 數值標準化（ Standardarization）
(D) Z-分數正規化（ Z-score Normalization）

212 分級
B 212. 胖虎目前在分析一間公司的健康檢查資料，其中有一個欄位是 BMI 值，胖虎想要將其根據不同區段分類為過輕、正常、過胖、肥胖，請問胖虎正在做的是何種屬性轉換？
(A) 二值化（ Binarization）
(B) 分級（ Bining）
(C) 捨入（ Rounding）
(D) Log 轉換（ Log Transformation）

12 屬性變換離散化
C 12. 下列何者可對連續變量進行離散化（ Discretization）處理？
(A) 單熱編碼（ One-Hot Encoding）
(B) 標準化（ Standardization）
(C) 資料分箱（ Binning）
(D) 正規化（ Normalization）

111 類別變數轉換
A 111. 如果要使用邏輯式迴歸（ Logistic Regression）建立客戶流失分類模型，則我們必須將標籤（例如名為：流失，不流失）做什麼處理？
(A) 轉換為二元（ Binary）數值
(B) 必須將標籤從中文名稱轉換成英文 Yes 與 No
(C) 保留原始標籤名稱即可
(D) 必須將標籤轉換為數字型態

161 虛擬編碼
A 161. 請問經過下列轉換方法做資料轉換後，哪個方法會產生出較原資料多的變數？
(A) 虛擬編碼（ Dummy Encoding）
(B) 降低維度（ Dimension Reduction）
(C) 特徵縮放比例（ Feature Scaling）
(D) 特徵選擇（ Feature Selection）

162  One-Hot Encoding
A 162. 有一個類別型變項名稱為「鞋子顏色」，值域（亦即所有可能出現的值）為{紅, 藍, 綠}，下列何者為正確的 One-Hot Encoding 方式（ 變項名稱：值域）？
(A) 鞋子_紅:{1,0} , 鞋子_藍:{1,0}, 鞋子_綠:{1,0}
(B) 鞋子顏色:{紅, 藍, 綠}
(C) 鞋子顏色:{1, 2, 3}
(D) 鞋子_紅:{1, 2} , 鞋子_藍:{1, 2}, 鞋子_綠:{1, 2}

13 One-Hot Encoding
D 13. 參考附圖，是使用下列何者編碼方式對類別資料進行轉換？
商品 價格=>A B C 價格
A 29 1 0 0 29
B 24 0 1 0 24
C 32 0 0 1 32
(A) 頻率編碼（ Frequency Encoding）
(B) 序號編碼（ Ordinal Encoding）
(C) 標籤編碼（ Label Encoding）
(D) 單熱編碼（ One-Hot Encoding）

163 類別標籤
A 163. 下列哪個方法需要類別標籤（ Label）資訊？
(A) 線性判別分析（ Linear Discriminant Analysis）
(B) 主成分分析（ Principle Component Analysis）
(C) 潛在語意分析（ Latent Semantic Analysis）
(D) 獨立成分分析（ Independent Component Analysis）

210 傅立葉變換
A 210. 下列哪個方法是將時間序列資料轉換到頻域空間？
(A) 傅立葉轉換
(B) 特徵值加權
(C) 資料降維
(D) 隨機抽樣

14 Box-Cox變換
A 14. 關於 Box-Cox 轉換，下列敘述何者正確？
(A) 適用於當變數值恆正的時候
(B) 是一種線性轉換
(C) 可將對稱分佈的變數轉為偏斜分佈
(D) 只適用於右偏的變數分布


## 屬性萃取：屬性萃取指結合多個預測變數成為一個代理變數或稱為潛在變數

屬性萃取目的
- 降低資料維度
- 提高學習模型時的效率與效能
- 過濾無用資訊

屬性萃取功能
- 減少運算時間與儲存空間
- 移除共線性資料能有效提高線性模型的效能
- 當資料維度降至 2～3 維時，能很容易的直接視覺化展示資料分佈


### 主成份分析(PCA)
- 降維後運用二維或三維散佈圖視覺化多變量資料
- 將攸關的訊息與無關的雜訊隔離
- 將問題中的變數組合成數個具有訊息力的特徵變數
- 將高度相關的預測變數轉換成無關且量少的變數集，有利於某些方的的建模。

### 奇異值分解(SVD)
- 用運用在維度縮減

### 多維尺度分析（ Multidimensional Scaling）

#### 練習題
64特徵萃取
D 64. 下列何者不是特徵萃取所要達到的目的？
(A) 降低資料維度
(B) 提高學習模型時的效率與效能
(C) 過濾無用資訊
(D) 評估學習得到的模型效能

40 特徵萃取（ Feature Extraction）
B 40. 特徵萃取（ Feature Extraction）是指將原始資料的屬性進行結合，以產生新的代理變數（ Surrogate Variables）。下列常用的降維方法中，何者屬於特徵萃取的方式？
(A) 低變異過濾（ Low Variance Filter）
(B) 多維尺度分析（ Multidimensional Scaling）
(C) 隨機森林（ Random Forests）
(D) 高相關過濾（ High Correlation Filter）


183屬性萃取
C 183. 請問我們可以使用哪一種方法進行屬性萃取？
(A) 交替最小次方法（ Alternating Least Square, ALS）
(B) 二元搜索樹（ Binary Search Tree）
(C) 主成分分析（ Principal Component Analysis, PCA）
(D) K 平均法（ K-Means）


13屬性萃取
C 13. 下列何者較「不適合」 用來作為屬性萃取（ feature extraction） 的方法？
(A) 主成分分析（ principal component analysis）
(B) 拉普拉斯特徵映射法（ Laplacian eigenmaps）
(C) 交叉驗證（ cross validation）
(D) 自組織映射圖（ self-organizing map）

11降維
A 11. 在機器學習中有多種降低資料維度的方法，下列何者屬於降維度的方法？
(A) 主成份分析（ Principal Component Analysis）
(B) 決策樹（ Decision Tree）
(C) K-近鄰演算法（ K Nearest Neighbor）
(D) 羅吉斯迴歸（ Logistic Regression）


14降維
D 14. 下列何者不是降維的好處？
(A) 減少運算時間與儲存空間
(B) 移除共線性資料能有效提高線性模型的效能
(C) 當資料維度降至 2～3 維時，能很容易的直接視覺化展示資料分佈
(D) 降維後的資料集訊息量增加，不會減少

65降維
D 65. 下列何者不是常見的資料維度降維方法？
(A) 主成分分析（ Principle Component Analysis）
(B) 核主成分分析（ Kernel PCA）
(C) 多維尺度法（ Multidimensional Scaling）
(D) K 平均法（ K-means）

232降維
C 232. 下列何者不是資料降維的方法？
(A) Principal Component Analysis
(B) Linear Discriminant Analysis
(C) K Nearest Neighbors
(D) Isomap

62主成分分析
A 62. 使用下列何種方法，可以重新組合資料屬性，產生新的維度？
(A) 主成分分析法（ PCA， Principle Component Analysis）
(B) K 平均法（ K-means）
(C) C50
(D) 卡方檢定（ Chi-square test）

31 PCA
D 31. 關於主成分分析（ Principal Component Analysis, PCA） 屬性萃取的主要用途，下列敘述何者正確？
(A) 以長條圖視覺化多變量資料
(B) 將低度相關的預測變數矩陣 x，轉換成相關且量多的潛在變項集合
(C) 將最攸關的訊息與無關的雜訊結合
(D) 將問題領域中的數個變數，組合成單一或數個具訊息力的特徵變數


C 4. 用主成分分析（ principle component analysis） 來處理多維度資料時，會利用相關矩陣（ correlation matrix） 來計算特徵值（ eigenvalue） 與特徵向量（ eigenvector），如果特徵向量 λ= [4.32, 1.07, 0.49, 0.10, 0.01, 0.01]，下列敘述何者正確？
(A) 主特徵值的貢獻率達到 80%
(B) 主特徵值的貢獻率達到 90%
(C) 前兩個特徵值的貢獻率達到 90%
(D) 前兩個特徵值的代表性不足

32SVD
B 32. 關於奇異值分解（ Singular Value Decomposition, SVD），下列敘述何者「不正確」？
(A) 常見應用是維度縮減
(B) 用於估計結果穩定時
(C) 用於資料其屬性個數大於觀測值個數
(D) 用於估計結果不穩定時

15屬性萃取
A 15. 在文字探勘技術中，為了使文字資料轉換為電腦看得懂的數值資料，資料科學家建立一個二維結構，其中屬性為字典中所有字詞，而屬性的數量為字典中的詞彙數量。每一篇文章經過斷詞之後，會在此結構中建立一筆紀錄， 判斷每一個字詞在各文章中是否出現。 請問此技術運用了下列何種屬性萃取（ feature extraction） 的方法？
(A) 單熱編碼（ one-hot encoding）
(B) 分級裝箱（ binning）
(C) 四捨五入（ rounding）
(D) 對數轉換（ log transformation）

## 屬性挑選：選則選擇移除訊息貧瘠、贅餘或無關的變數。

### 挑選的目的：
- 提高機器學習效能(提升模型績效)
- 篩選最佳屬性
- 避免過度配適（ Overfitting）

### 常用方法：
- 遺缺值比例刪除觀察值
- 移除分佈異常(退化或過度分散)之變數
- 屬性排名法(feature ranking)：依據某個準則，例如：變異數、卡方統計量、訊息增益、相關係數、percentUnique、freqRatio等
  - 過濾法(filter)：為非監督式的，單純的就預測變數空間選取屬性子集在進行建模
    - 例如：高相關預測變數移除、主成份迴歸PCR、訊息增益與增益率、退化分佈指數
  - 封裝法(wrapper)：結合模型績效指標來挑選數性子集合
    - 例如：多元迴歸中的逐步迴歸
  - 內嵌法(embedded)：許多監督式學習方法有內嵌的屬性挑選機制，它們在學習的過程中試著找出最佳的屬性子集
    - 例如：脊迴歸、套索迴歸、樹狀模型建模方法

- percentUnique：獨一無二的類別數值數量與樣本大小的比集，如果太高代表類別變數的互異類別值非常多，宜刪除此類準識別欄位。
- freqRatio：最頻繁的類別值頻次除以次頻繁類別變數值頻次的比值。如果太高代表類別變數高度集中在最頻繁的類別，宜刪除此高度偏斜變數。

#### 練習題
113 特徵選取目的
D 113. 下列何者並非屬性選取（ Feature Selection）的目的？
(A) 避免過度配適（ Overfitting）
(B) 提高機器學習效能
(C) 篩選最佳屬性
(D) 提高資料維度

15維度縮減
D 15. 下列何項不是迴歸分析常用的維度縮減技術？
(A) 係數縮減法（ Shrinkage）
(B) 逐步迴歸法（ Stepwise Regression）
(C) 子集挑選法（ Subset Selection）
(D) 事後修剪法（ Post-pruning）

137屬性挑選
C 137. 屬性挑選（ Feature Selection）是指挑選原始資料中的合宜屬性，或可視為移除缺乏訊息內涵之變數的維度縮減策略。下列常用的降維方法中，何者不屬於屬性挑選的方式？
(A) 遺缺值比率（ Missing Values Ratio）
(B) 前向式屬性構模（ Forward Feature Construction）
(C) 主成分分析（ Principal Component Analysis）
(D) 卡方檢定與信息增益（ Chi-square and Information Gain）

164 屬性挑選
A 164. 關於屬性挑選，下列敘述何者正確？
(A) 過濾式屬性挑選法（ Filter）單就預測變數空間中進行挑選工作
(B) 封裝式屬性挑選法（ Wrapper）不考慮後續建模方式的方法
(C) 大數據（ Big Data）時代下，用越多屬性詮釋反應變數越好
(D) 屬性挑選時通常僅需考慮個別屬性的分佈狀況，無須考慮屬性間的互動關係

165 屬性挑選
C 165. 若您在分析資料後發現，由於資料的某些欄位具有高度相關性而影響了分析結果，請問可能是忘了進行下列哪一步驟？
(A) 清理資料
(B) 轉換資料結構
(C) 屬性挑選
(D) 蒐集的資料不正確

214特徵選擇
D 214. 下列哪種方法不屬於特徵選擇（ Feature-Selection） 的標準方法？
(A) 嵌入方法（ Embedded）
(B) 過濾方法（ Filter）
(C) 包裝方法（ Wrapper）
(D) 抽樣方法（ Sampling）

15 屬性挑選
C 15. 附圖是移除預測變數流程中的步驟，下列何者為正確的排序？
1：計算 A 與其他變數間的相關係數平均值， B 亦同
2：如果 A 有較大的平均相關係數，則刪除之。否則，請刪除 B 變數
3：找出相關係數絕對值最大的兩個預測變數 A 與 B
4：重複上述三個步驟，直到沒有相關係數的絕對值超出門檻
5：計算預測變數的相關係數矩陣，並設定相關係數的絕對值門檻
(A) 1＞2＞3＞4＞5
(B) 5＞1＞2＞3＞4
(C) 5＞3＞1＞2＞4
(D) 1＞5＞3＞4＞2

39 特徵挑選
D 39. 特徵挑選（ Feature Selection） 是指挑選原始資料中的合宜屬性，或可視為移除缺乏訊息內涵之變數的維度縮減策略， 下列常用的降維方法中， 何者屬於特徵挑選的方式？
(A) 因子分析（ Factor Analysis）
(B) 非負矩陣分解（ Non-negative Matrix Factorization）
(C) 隨機投影（ Random Projections）
(D) 高相關過濾（ High Correlation Filter）


28freqRatio  percentUnique
D 28. 某預測變數有兩個獨一無二的值，假設有 1000 個樣本，其中 999 個樣本的預測變數值相同，下列敘述何者「不正確」 ？
(A) 類別型預測變數與數值型預測變數的退化分佈辨識方法不盡相同
(B) freqRatio 的值為 999，它是以最常見的類別值頻次，除以次常見類別值頻次的比值 (999/1)
(C) percentUnique 的值為 0.002，它是以獨一無二的類別值數量與樣本大小的比值 (2/1000)
(D) 此變數不屬於近乎零變異（ near-zero variance） 的狀況

30 freqRatio與 percentUnique
B 30. 關於類別型變數的頻繁次數比（ Frequent ratio）與唯一值百分比（ Percentunique）資料相關性，下列敘述何者「不正確」？
(A) 頻繁次數比之數值如果太大，表示此變數集中出現最頻繁類別，可考慮刪除此類別型變數
(B) 某類別型變數計次結果為男： 2 次，女： 98 次，則頻繁次數比為98 (98/2)
(C) 唯一值百分比之數值太大，表示此變數幾乎完全相同，可考慮刪除此類別型變數
(D) 某類別型變數計次結果為男： 2 次，女： 98 次，則唯一值百分比為 2   (2/100)

- percentUnique：獨一無二的類別數值數量與樣本大小的比值，如果太高代表類別變數的互異類別值非常多，宜刪除此類準識別欄位。
- freqRatio：最頻繁的類別值頻次除以次頻繁類別變數值頻次的比值。如果太高代表類別變數高度集中在最頻繁的類別，宜刪除此高度偏斜變數。


A 175. 預測建模前經常會先以各式統計量數，移除無用的預測變數，下列敘述何者正確？
(A) 數值型預測變數可以其間的相關係數，剔除贅餘的預測變數
(B) 線性迴歸模型中納入退化分佈（ Degenerate Distribution） 的預測變數，並不會損傷其績效
(C) percentUnique 是以最常見的類別值頻次，除以次常見類別值頻次的比值，來辨識有退化分佈現象的類別型變數
(D) freqRatio 是以獨一無二的類別值數量與樣本大小的比值，來辨識有退化分佈現象的類別型變數


188 屬性轉換與資料縮減
A188. 屬性轉換（ Feature Transformation）與資料縮減（ Data Reduction）屬於資料前處理（ Data Preprocessing）的重要工作，下列敘述何者不正確？
(A) 線性迴歸、偏最小平方法（ PLS）、類神經網絡（ NN）等算法內嵌有變數選擇機制的方法，對於預測變數中的雜訊，或是無訊息力的變數等較不敏感
(B) 文字資料探勘（ Text Mining）中的詞頻-逆文件頻率可視為維度縮減
(C) 詞組提取（ Chunk Extraction）與 N 元（ N-gram）字組，算是文字資料探勘的降維方法
(D) 選用的模型種類決定資料前處理的需求

92 屬性選擇指標
B 92. 不同的決策樹方法，我們可以透過屬性選擇指標（ Attribute Selection Measure），將資料分割成個別類別，使其所包含的資料群組具有相同的類別，試問下列何者不是屬性選擇指標？
(A) 資訊獲利（ Information Gain）
(B) 拉普拉斯估計式（ Laplace Estimator）
(C) 獲利比率（ Gain Ratio）
(D) 吉尼係數（ Gini Index）

136 多變量探索式分析
B 136. 關於多變量探索式分析（ Multivariate Exploratory Analysis），下列敘述何者不正確？
(A) 多個量化變數（ Quantitative Variables）之間的關係，通常可以散佈圖矩陣（ Scatterplot Matrix）來表達
(B) 共變異數（ Covariance）是相對指標（ Relative Index），而相關係數（ Correlation Coefficient）是絕對指標（ Absolute Index）
(C) 計算數值資料矩陣中所有成對的相關係數，即可獲得相關係數矩陣（ Correlation Matrix）
(D) 透過數據排名值（ Rank）的方式，將共變異數與相關係數的概念延伸到類別變數上
