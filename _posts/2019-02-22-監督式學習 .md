---
layout      : post
title       : 監督式學習
date        : 2019-02-22 10:41:10 +0800
categories  : 資料處理與分析概論
tags        : [資料分析, 機器學習概論,監督式學習 ]
---

# 2-2 監督式學習

回歸與分類問題
- 線性模型
  - 線性迴歸
  - 邏輯迴歸（ Logistic Regression）
- 非線性模型
  - K 近鄰法（ K-Nearest Neighbor）
  - 支援向量機（ Support Vector Machine）
  - 類神經網路（ Neural Network）

#### 練習題
247 監督式學習
D 247. 下列何者不為監督式學習（ Supervised Learning） 方法？
(A) K 近鄰法（ K-Nearest Neighbor）
(B) 支援向量機（ Support Vector Machine）
(C) 邏輯迴歸（ Logistic Regression）
(D) 自我組織映像圖（ Self-OrganizingMap）

98監督式學習
C 98. 下列何種機器學習方法不屬於監督式學習演算法？
(A) 邏輯迴歸（ Logistic Regression）
(B) 類神經網路（ Neural Network）
(C) 多維尺度法（ Multidimensional Scaling）
(D) 最近鄰居法（ Nearest Neighbor）


46機器學習
B 46. 下列何種統計機器學習方法，容許資料中存有遺缺值？
(A) 類神經網路（ Artificial Neural Networks）
(B) 分類與迴歸樹（ Classification and Regression Trees）
(C) K-近鄰法（ K-Nearest Neighbors）
(D) 羅吉斯迴歸（ Logistic Regression）

249 監督式學習
C 249. 關於監督式學習， 下列敘述何者不正確？
(A) 模型的訓練資料必須有應變項
(B) 訓練資料不一定為連續型資料
(C) 主成分分析是一種監督式學習的方法
(D) 訓練資料過少時，可利用 Bootstrap（拔靴法）進行修正

48半監督式學習
C 48. 假設建立一個能夠辨識汽車的模型系統，在照片資料集共有 100 萬張照片，其中有 1000 張已標註汽車貼標的照片，接下來可用哪種學習方法找出剩下的照片當中是否有汽車？
(A) 監督式學習（ Supervised learning）
(B) 非監督式學習（ Unsupervised learning）
(C) 半監督式學習（ Semi-supervised learning）
(D) 增強式學習（ Reinforcement learning）


## 回歸與分類問題

- 迴歸：利用自變數(預測變數x)來預測依變數(反應變數y)未來可能產生之值(數值)
- 分類：利用自變數(預測變數x)來預測依變數(反應變數y)未來可能產生之值，進一步地用於分類(類別)

#### 練習題
147 分析方法
D 147. 選用線性模型或統計分析方法時，會根據依變數與自變數的屬性而有
所不同，請問下列表格與配對的選項，何者不適當？
依變數 - 連續型 依變數 - 離散型
自變數 - 連續型 (A)線性迴歸 (B)線性判別分析（ Linear Discriminant Analysis）
自變數 - 離散型 (C)變異數分析 (D)皮爾森相關係數（ Pearson Correlation Coefficient）

125簡單線性迴歸
A 125. 考慮簡單線性迴歸（ Linear Regression）模型，其變異數分析表中，迴歸模型的自由度為何？
(A) 1
(B) 2
(C) 12
(D) 20

44迴歸分析
D 44. 關於迴歸分析的基本統計假設，下列敘述何者正確？
(A) 依變數和自變數之間的關係必須是線性
(B) 資料呈現常態分配（ Normal Distribution）
(C) 自變數的誤差項，相互之間應該是獨立的
(D) 以上皆是

47前向式逐步迴歸
B 47. 當資料集的預測變數過多時，下列哪種方法是從只有截距項的最簡單模型出發，逐步加入重要的變數？
(A) 後向式逐步迴歸
(B) 前向式逐步迴歸
(C) 中向式逐步迴歸
(D) 反覆式逐步迴歸

42分類預測
B 42. 下列何種演算法較「不適合」進行分類預測？
(A) 決策樹（ Decision Tree）
(B) 線性迴歸（ Linear Regression）
(C) 羅吉斯迴歸（ Logistic Regression）
(D) K-近鄰演算法（ K-Nearest Neighbor）

44迴歸與分類
B 44. 迴歸問題和分類問題都屬於監督式學習，關於兩者的反應變數，下列敘述何者正確？
(A) 前者是類別型反應變數，後者是數值型反應變數
(B) 前者是數值型反應變數，後者是類別型反應變數
(C) 兩者都是數值型反應變數
(D) 兩者都是類別型反應變數

45迴歸模型
A 45. 關於迴歸模型，下列敘述何者「不正確」 ？
(A) 可用來解釋資料現象間的因果關係
(B) 利用自變數來預測依變數未來可能產生之值
(C) 視其函數之型態分為線性與非線性
(D) 根據自變數個數可分為簡單迴歸分析（ simple regression analysis）及複迴歸分析（ multiple regression analysis）

241 分類問題
B 241. 下列何種方法常應用在分類問題？
(A) Linear regression
(B) Logistic regression
(C) Polynomial Regression
(D) Support vector regression


198 分類模型
A 198. 下列何者不適合用來「 預測明天會不會下雨」 ？
(A) 支援向量迴歸（ Support Vector Regression）
(B) 支援向量機（ Support Vector Machine）
(C) 決策樹（ Decision Tree）
(D) 類神經網路（ Neural Network）

43分類模型
B 43. 若希望能透過學生基本資料與參與社團資料，來預測新生會選擇的社團，運用以下何種工具較為適當？
(A) 線性迴歸模型
(B) 分類模型
(C) 集群分析
(D) 探索式分析

142 分類模型
B 142. 下列何者為「常用來分析客戶資料，並建構模型以預測顧客流失（ Churn）與否」的方法？
(A) 關聯規則分析
(B) 分類模型
(C) 迴歸分析
(D) 集群分析

94 線性迴歸模型
A 94. 若希望能透過歷史氣溫與菜價資料來預測未來菜價，運用以下何種工具較為適當？
(A) 線性迴歸模型
(B) 分類模型
(C) 集群分析
(D) 探索式分析

43多元迴歸
B 43. 請問若只需輸入大學生的身高和體重來預測其腰圍，使用何種演算法較為合適？
(A) 簡單線性迴歸
(B) 多元線性迴歸
(C) 羅吉斯迴歸
(D) 關聯規則

A 8. 分類問題當不同類的樣本數不平衡時，下列何者「 不是」 處理方式？
(A) 使用丟棄（ dropout） 方法從大類中剔除一些樣本
(B) 使用降抽樣（ undersampling） 方法從大類中選取部分樣本
(C) 使用權重（ weighting） 方法調整樣本權重
(D) 使用數據合成（ synthetic） 方法生成新的樣本

D 30. 下列何者通常「 不」 用來處理連續值的預測問題？
(A) 簡單線性迴歸（ simple linear regression）
(B) 多元迴歸分析（ multiple regression analysis）
(C) 支援向量迴歸（ support vector regression）
(D) 羅吉斯迴歸（ logistic regression）

A 31. 下列何種方法，訓練的速度通常最快？
(A) k 近鄰法（ k-nearest neighbors）
(B) 支援向量機（ support sector machine）
(C) 決策樹（ decision tree）
(D) 多層感知器（ multilayer perceptron）

## 線性模型

### 線性模型假設
- 依變項（要預測的變項）呈現常態分佈
- 自變項（預測變項）之間是獨立事件
- 依變項之誤差為常態分佈

#### 練習題
193 線性機器學習
A 193. 下列何者屬於線性的機器學習模型？
(A) 偏最小平方法（ Partial Least Squares）
(B) 貝氏分類法（ Naïve Bayes）
(C) 支援向量機（ Support Vector Machine）
(D) 類神經網路（ Neural Network）

146 線性迴歸
B 146. 在線性迴歸中，以最小平方法計算迴歸參數時，殘差需符合四大條件。下列何者非屬四大條件之一？
(A) 殘差期望值為零
(B) 殘差必須符合均勻分配
(C) 殘差之間沒有自相關（殘差獨立性）
(D) 殘差需符合變異數同質性

144 線性迴歸模型
C 144. 關於線性迴歸模型（ Linear Regression）的假設，下列敘述何者不正確？
(A) 依變項（要預測的變項）呈現常態分佈
(B) 自變項（預測變項）之間是獨立事件
(C) 自變項的組合來自隨機抽樣（例如五個自變項隨機抽三個出來做迴歸）
(D) 依變項之誤差為常態分佈

246 線性模型
A 246. 下列哪種的資料可以無需經過前處理， 直接使用線性模型（ Linear Model）進行學習？
(A) 身高（公分）、體重（公斤）
(B) 性別（男、女）、腰圍（公分）
(C) 最高時速（公里/小時）、車款（車種型號）
(D) 氣候（晴、陰、雨）、溫度（攝氏溫度）


200 線性迴歸
D 200. 關於線性迴歸，下列敘述何者不正確？
(A) 並非任何資料集均可建立多元線性迴歸模型（ multiple linear regression），有時會有建模失敗的狀況發生
(B) 線性迴歸屬於無母數（ nonparametric）的統計建模方法
(C) 迴歸方程式係數估計最佳化問題是最小化誤差平方和（ Sum of Squared Error, SSE）
(D) 相較於類神經網路與支援向量機等監督式學習模型，迴歸建模所獲得的模型可解釋性較低


50線性模型
B 50. 當使用線性模型時，哪種方法對於學習預測線性不可分的資料集也許有幫助？
(A) 交叉驗證（ Cross validation）
(B) 核方法（ Kernel method）
(C) 過採樣（ Over sampling）
(D) 降採樣（ Down sampling）

148 資料轉換
D 148. 根據資料分佈選擇模型假設時，會遇到不適合直接建立線性模型的情況，此時可以透過轉換資料的方式進行修正。請問透過何種模型，適合將圖 1 的資料分佈轉換為圖 2？
圖 1 圖 2
(A) y=β0+β1 x
(B) y=β0+β1 x+β2 x2
(C) y2=β0+β1 x
(D) log(y)= β0+β1 log(x)

191 多元迴歸與廣義線性模型
A 191. 關於多元迴歸（ Multiple Regression）與廣義線性模型（ Generalized Linear Model， GLM），下列敘述何者不正確？
(A) 多元迴歸的自變項必須是類別資料
(B) GLM 的自變項可以是類別資料
(C) 多元迴歸的應變項必須為單一個
(D) GLM 的自變項可以是兩個以上


### 羅吉斯模型

48羅吉斯迴歸
C 48. 關於羅吉斯迴歸（ Logistic Regression）分類，下列敘述何者「不正確」？
(A) 它是建立二元類別機率值之勝率（ odds ratio） 對數值的線性分類模型
(B) 其反應變數假設是二項式隨機變數
(C) 它對反應變數直接建模，將之關連到預測變數的線性函數
(D) 常被人誤解成數值迴歸技術


D 49. 下列何者情況不適合使用邏輯迴歸（ Logistic Regression）模型？
(A) 明天是否下雨
(B) 鐵達尼號乘客是否存活
(C) 顧客是否會購買週年慶商品
(D) 行動通訊用戶國際電話服務用量預測

225 邏輯斯迴歸
A 225. 關於邏輯斯迴歸中的迴歸係數， 可以使用下列何種方法求解？
(A) 最小平方法
(B) 牛頓迭代法
(C) 馬可夫鏈演算法
(D) 最大概似估計法


## 非線性模型
- 天真貝氏分類
- k近鄰法
- 支援向量機
- 決策樹

#### 練習題
192 KNN
D 192. 關於 K 近鄰法（ K-Nearest Neighbor），下列敘述何者不正確？
(A) 根據現有已分類好的資料集合，找出 K 個與待分類資料最鄰近的現存資料
(B) 根據 K 個最鄰近資料之類別，以多數決的方式決定待分類資料的所屬類別
(C) 建議 K 為奇數值
(D) 根據 K 值將資料分為 K 群

42Eager Leaner
C 42. 關於熱切式學習（ Eager Learner）與偷懶式學習（ Lazy Learner），下列敘述何者不正確？
(A) 熱切式學習是先利用訓練資料建立一個判別模型，以便進行測試
(B) 決策樹屬於熱切式學習
(C) 偷懶式學習會花很多時間在事先利用訓練資料建立判斷模型
(D) k-最近鄰分類法（ K-Nearest-Neighbor Classifiers）屬於偷懶式學習



141 非線性高微空間SVM
A 141. 下列何種方法，最適合解決非線性分類問題及高維空間數據識別問題？
(A) 支援向量機（ Support Vector Machine）
(B) K 平均法（ K-means）
(C) 貝式判別（ Bayesian Classifier）
(D) 模糊理論（ Fuzzy Set）

196 SVM
C 196. 支援向量機（ Support Vector Machine） 源自最大邊界分類法（ Maximun
Margin Classifier）， 請問哪一條線是最大邊界分類法建立的決策邊界？


49係數正規化
C 47. 關於決策樹（ decision tree） 演算法， 下列何者不侷限使用於離散型資料？
(A) ID3
(B) CHAID
(C) CART
(D) C4.5

194 樹狀模型
A 194. 關於常用於資料建模的演算法，下列敘述何者不正確？
(A) K 平均法（ K-Means）演算法是分類演算法的一種
(B) ID3（ Iterative Dichotomiser 3）演算法是決策樹演算法的一種
(C) C4.5 演算法是分類演算法的一種
(D) CART（ Classification and Regression Trees）演算法是分類演算法的一種

41決策樹
D 41. 下列何者不是決策樹產生的基本演算法？
(A) ID3（ Iterative Dichotomiser）
(B) C4.5
(C) CART（ Classification and Regression Trees）
(D) 貝氏分類（ Bayesian Classification）


92決策樹
B 92. 不同的決策樹方法，我們可以透過屬性選擇指標（ Attribute Selection Measure），將資料分割成個別類別，使其所包含的資料群組具有相同的類別，試問下列何者不是屬性選擇指標？
(A) 資訊獲利（ Information Gain）
(B) 拉普拉斯估計式（ Laplace Estimator）
(C) 獲利比率（ Gain Ratio）
(D) 吉尼係數（ Gini Index）

C 27. 關於決策樹（ decision tree）的使用， 下列敘述何者「不」正確？
(A) 每次分割要讓子節點不純度降低
(B) 樹葉節點中的樣本應屬於同一類別
(C) 為了進行準確的預測，要讓決策樹盡量長大長深
(D) 結合多個決策樹形成隨機森林（ random forest），模型較穩健，不易過度配適


## 類神經網絡
- 類神經網絡
- 卷積神經網絡
- 遞歸神經網絡
- 自動編碼器
- 深度學習

#### 練習題
42多層感知機
A 42. 下列學習方法，何者「 難以」 獲得人類容易理解的知識或特徵？
(A) 多層感知機（ multilayer perceptron）
(B) 決策樹（ decision tree）
(C) 羅吉斯迴歸（ logistic regression）
(D) 關聯規則探勘（ association rule mining）

243 Multilayer perceptron
A 243. 下列學習方法，何者難以獲得人類容易理解的知識或特徵？
(A) Multilayer perceptron
(B) Decision tree
(C) Logistic regression
(D) Association rule mining

A 33. 關於類神經網路（ artificial neural networks）， 下列敘述何者「 不」正確？
(A) 類神經網路至少包括投入層與隱藏層，投入層各個節點接收資料表中的自變數， 隱藏層則試圖預測因變數
(B) 除了投入層之外，其餘各層的神經元節點均需設置活化函數（ activation function）
(C) 最簡單的類神經網路是感知機（ perceptron），它是一種線性分類模型
(D) 近年來一些分段線性活化函數漸受歡迎，整流線性單元（ RectifiedLinear Unit, ReLU） 和硬式雙曲正切函數（ hard hyperbolic tangent function） 大範圍地取代了 S 型函數（ Sigmoid function） 與雙曲正切函數（ hyperbolic tangent function），因為兩者更適合訓練多層神經網路


91類神經網路架構
B 91. 試問下列哪一項不包含在一個「多層前向式（ Multilayer Feed-Forward）」類神經網路架構？
(A) 輸入層（ Input Layer）
(B) 實體層（ Physical Layer）
(C) 隱藏層（ Hidden Layer）
(D) 輸出層（ Output Layer）

14深度學習
D 14. 深度學習（ deep learning） 中，會透過自編碼器（ AutoEncoder, AE） 來對影像資料降維。 關於自編碼器，下列敘述何者「不正確」 ？
(A) 去除影像中的雜訊
(B) 更加準確的進行影像分類
(C) 可用來產生新的影像結構
(D) 為一種監督式學習（ supervised learning）

195 深度學習
D 195. 深度學習（ Deep Learning）和下列哪一個演算法沒有直接相關？
(A) 卷積神經網路（ Convolutional Neural Networks）
(B) 遞歸神經網路（ Recurrent Neural Network）
(C) 感知學習演算法（ Perceptron Learning Algorithm）
(D) 關聯規則演算法（ Apriori Algorithm）

## 增強式學習
99 增強式學習
D 99. 假設行動廣告推薦系統，透過每次廣告推薦的決策中，得到用戶點擊與否的資訊，並不斷進行改進，修正其策略以得到最佳的廣告推薦效果，適合下列何種學習方法？
(A) 監督式學習（ Supervised Learning）
(B) 非監督式學習（ Unsupervised Learning）
(C) 半監督式學習（ Semi-supervised Learning）
(D) 增強式學習（ Reinforcement Learning）

242 自然語言分析
C 242. 下列何者不適合用來預測「 句子的下一個詞」 ？
(A) Hidden Markov model
(B) Conditional random field
(C) Linear Regression
(D) N-gram

## 薈萃式學習
- 拔靴集成
- 多模型激發法
- 隨機森林

#### 練習題
100 隨機森林
D 100. 關於隨機森林（ Random Forest），下列敘述何者不正確？
(A) 包含多個決策樹
(B) 可以幫助篩選重要的自變項
(C) 輸出可以採用投票或是平均
(D) 依變項只能使用類別型變項

B 46. 關於機器學習的建模方式， 下列何者比較不會因為增加模型的複雜度而產生過度配適（ overfitting） 的現象？
(A) 多層感知機（ multi-layer perceptron）
(B) 隨機森林（ random forest）
(C) 決策樹（ decision tree）
(D) 羅吉斯迴歸（ logistic regression）

C 47. 關於隨機森林（ random forest）， 下列敘述何者「不」正確？
(A) 可用於具有遺缺值的資料
(B) 可用來進行預測、分類
(C) 在薈萃式學習（ ensemble learning） 裡面，隨機森林採用效能提升的方式（ boosting） 進行系集模型的建構
(D) 提供變數重要度分數


D 48. 關於決策樹（ decision tree） 與隨機森林（ random forest） 的比較， 下列敘述何者正確？
(A) 決策樹屬於監督式學習（ supervised learning），隨機森林屬於非監督式學習（ unsupervised learning）
(B) 兩者皆屬於薈萃式學習（ ensemble learning）
(C) 隨機森林的每一棵決策樹之間是有關聯的
(D) 隨機森林能處理離散型資料，也能處理連續型資料

245 薈萃式學習
A 245. 利用多個分類器的預測來提高分類的準確率之技術為下列何者？
(A) Ensemble
(B) Dimensionality reduction
(C) Pruning
(D) Feature selection

C 36. 以拔靴集成法（ Bootsrap AGGregatING, BAGGING） 產生的 5 株裝袋樹（ bagged tree），對某個觀測值的預測結果分別 1,0, 0, 0, 1，請問最終預測結果為下列何者？
(A) 1
(B) 3
(C) 0
(D) 2

A 37. 假設有一訓練資料集為 S = {1(-), 1(-), 2(+), 2(+), 2(+), 3(-), 3(-), 5(-), 7(-),
9(-)}，其中數字代表特徵值，括號內的正負號代表類別。若使用拔靴集成法（ Bootsrap AGGregatING, BAGGING） 建立分類器時，使用的抽樣方法為下列何者？ （ 假設抽樣的資料筆數亦為 10）
(A) 隨機從 S 中抽取 10 筆資料， S 中的每筆資料可被抽取不只一次，例如： {5(-), 1(-), 9(-), 7(-), 9(-), 3(-), 1(-), 2(+), 5(-), 2(+)}
(B) 隨機從 S 中抽取 10 筆資料， S 中的每筆資料至多只能抽取一次，例如： {5(-), 1(-), 9(-), 2(+), 3(-), 2(+), 7(-), 2(+), 3(-), 1(-)}
(C) 隨機從 S 中抽取 10 筆資料， S 中的每筆資料可被抽取不只一次，但必須維持正負兩個類別的比例各半(1:1)，例如： {2(+), 1(-), 9(-),2(+), 2(+), 3(-), 2(+), 7(-), 2(+), 3(-)}
(D) 隨機從 S 中抽取 10 筆資料， S 中的每筆資料可被抽取不只一次，但必須維持正負兩個類別的比例和 S 中的比例相同(3:7)，例如：{2(+), 1(-), 9(-), 7(-), 9(-), 3(-), 1(-), 2(+), 5(-), 2(+)}

D 38. 俗話說：「三個臭皮匠，勝過一個諸葛亮」，恰好呼應了拔靴集成法（ Bootsrap AGGregatING, BAGGING） 中，會產生多個弱分類器，並將這些弱分類器組合在一起，以獲得一個強分類器，有機會做出更準確的判斷。請問以下敘述何者正確？
(A) 假如臭皮匠們沒有贏過諸葛亮，只要持續增加臭皮匠的人數，必定可以勝過諸葛亮。（ 亦即：假如強分類器表現不好，只要增加弱分類器的數量，必定能提升強分類器的分類準確度。 ）
(B) 每個臭皮匠的意見越接近越好，因為表示他們越團結一心。（ 亦即：各個弱分類器的判斷結果越接近，強分類器的分類結果越準確。 ）
(C) 增加臭皮匠的數量會導致意見分歧，反而無法贏過諸葛亮。（ 亦即：增加弱分類器數量，反而會導致強分類器的分類準確度變差。 ）
(D) 即使沒有一個臭皮匠能答對所有的問題，但這些臭皮匠一起討論後，仍有可能答對所有的問題。（ 亦即：雖然各個弱分類器的分類準確度都不到 100%，但強分類器的分類準確度仍有可能達到100%。 ）

A 39. 關於機器學習，下列敘述何者「不」正確？
(A) 薈萃式學習（ ensemble learning） 著眼於不同分類模型的特質，及其對訓練資料中隨機噪訊的不同敏感程度，它只能集合多個不同類模型的預測結果，總結出來的預測值能準確命中標的
(B) 薈萃式學習以重抽樣方法，產製多個基本模型（ base learner），成為共同決策的一系列模型
(C) 非監督式屬於問題定義尚未十分清楚時探索與知識發現的方法，其目的仍是為了建立更好的監督式學習模型
(D) 大數據時代下我們面臨的問題益形複雜，專家學者們在非監督式與監督式學習的基礎上，延伸了更多解決複雜問題的統計機器學習方式，薈萃式學習因應而生

A 40. 使用拔靴集成法（ Bootsrap AGGregatING, BAGGING） 需要注意的地方，「 不」 包含下列何者？
(A) 預先指定特徵進行訓練
(B) 拔靴集成法用抽樣資料建構的模型，可能有好有壞，所以通常以取平均（ 或投票法） 的方式，來獲得較穩定的表現
(C) 拔靴集成法能提升機器學習演算法的穩定性和準確性，它可以減少模型的變異數（ variance） 從而避免出現過度配適（ overfitting）的現象
(D) 拔靴集成法可以改良比較不穩定演算法的性能，例如：類神經網路、 CART

D 41. 一般說來， 拔靴集成法（ Bootsrap AGGregatING, BAGGING） 其集成模型中各株決策樹（ decision tree） 是__________；效能提升法（ boosting）其集成模型中各株樹則是__________。
(A) 經過修剪的（ pruned）；經過修剪的（ pruned）
(B) 經過修剪的（ pruned）；未經修剪的（ unpruned）
(C) 未經修剪的（ unpruned）；未經修剪的（ unpruned）
(D) 未經修剪的（ unpruned）；經過修剪的（ pruned）

D 42. 效能提升法（ boosting） 是將弱分類器（ weak classifiers） 集合起來，轉換為強分類器（ strong classifier）。請問下列敘述何者「不」正確？
(A) 弱分類器是指，僅比隨機亂猜好一點點的模型，例如：擲銅板
(B) 對於訓練好的弱分類器，可按照權重進行投票
(C) 自適應效能提升法（ Adaptive Boosting , AdaBoost） 方法，是一個具有即時調節觀測值抽樣權重的演算法
(D) 自適應效能提升法（ Adaptive Boosting , AdaBoost） 方法，會出現有過度配適（ overfitting）的情況

D 43. 下列何者「不是」 薈萃式學習（ ensemble learning） 常用的集成技術？
(A) 投票法（ voting）
(B) 平均法（ averaging）
(C) 提升法（ boosting）
(D) 平衡法（ balance）

A 44. 關於效能提升法（ boosting），下列敘述何者「不」正確？
(A) 又稱為層積法（ stacking）
(B) 前面模型預測不準的樣本，後續被抽出的機率增大，使得後面的模型加強對這些樣本作出更準確的預測，互補合作提升整體效能
(C) 效能提升之意是建立多個互補的弱模型（ weak learners），將之集成後發揮團結力量大的綜效
(D) 極端梯度多模激發法（ eXtreme Gradient BOOSTing, XGBOOST）結合排序算法與直方圖計算最佳的屬性分割值

D 45. 關於自適應效能提升法（ Adaptive Boosting , AdaBoost） 的訓練過程，下列敘述何者「不」正確？
(A) 如果有 M 個樣本，則每一個訓練樣本最開始時都被賦予相同的權值： 1/M
(B) 某個樣本點若已經被準確地分類，在建構下一個訓練集中，它的權重就被降低；若沒有被準確地分類，那麼它的權重就得到提高
(C) 將每個訓練得到的弱分類器（ weak classifiers） 集合成強分類器（ strong classifier）
(D) 分類錯誤率低的弱分類器（ weak classifiers） 在最終分類器中占的權重較小，否則較大

C 49. 自適應效能提升法（ Adaptive Boosting , AdaBoost）與隨機森林（ random forest） 的關鍵差異在於？
(A) 前者的基本模型是強模型（ strong learners）；後者則是集成數個弱模型（ weak learners）
(B) 前者的拔靴抽樣各樣本權重相同；後者則依前面模型預測結果的良窳，對各樣本進行加權抽樣
(C) 前者運用資料集中的全部屬性；後者只使用資料集中的部份屬性
(D) 前者的基本模型是樹狀模型（ tree-like models）；後者則是非樹狀模型

C 50. R 語言的 rpart 套件，實現了_________算法的諸多概念？
(A) 迭代二分樹第三代（ Iterative Dichotomiser 3, ID3）算法
(B) C4.5
(C) 分類與迴歸樹（ Classification and Regression Trees, CART）
(D) C5.0
