---
layout      : post
title       :  主成份分析
date        : 2020-04-22 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,主成份分析,非監督式學習]
---

# 主成份分析(PCA)
- 降維後運用二維或三維散佈圖視覺化多變量資料
- 將攸關的訊息與無關的雜訊隔離
- 將問題中的變數組合成數個具有訊息力的特徵變數
- 將高度相關的預測變數轉換成無關且量少的變數集，有利於某些方的的建模。

# 奇異值分解(SVD)
- 用運用在維度縮減

# 多維尺度分析（ Multidimensional Scaling）

#### 練習題
64特徵萃取
D 64. 下列何者不是特徵萃取所要達到的目的？
(A) 降低資料維度
(B) 提高學習模型時的效率與效能
(C) 過濾無用資訊
(D) 評估學習得到的模型效能

40 特徵萃取（ Feature Extraction）
B 40. 特徵萃取（ Feature Extraction）是指將原始資料的屬性進行結合，以產生新的代理變數（ Surrogate Variables）。下列常用的降維方法中，何者屬於特徵萃取的方式？
(A) 低變異過濾（ Low Variance Filter）
(B) 多維尺度分析（ Multidimensional Scaling）
(C) 隨機森林（ Random Forests）
(D) 高相關過濾（ High Correlation Filter）


183屬性萃取
C 183. 請問我們可以使用哪一種方法進行屬性萃取？
(A) 交替最小次方法（ Alternating Least Square, ALS）
(B) 二元搜索樹（ Binary Search Tree）
(C) 主成分分析（ Principal Component Analysis, PCA）
(D) K 平均法（ K-Means）


13屬性萃取
C 13. 下列何者較「不適合」 用來作為屬性萃取（ feature extraction） 的方法？
(A) 主成分分析（ principal component analysis）
(B) 拉普拉斯特徵映射法（ Laplacian eigenmaps）
(C) 交叉驗證（ cross validation）
(D) 自組織映射圖（ self-organizing map）

11降維
A 11. 在機器學習中有多種降低資料維度的方法，下列何者屬於降維度的方法？
(A) 主成份分析（ Principal Component Analysis）
(B) 決策樹（ Decision Tree）
(C) K-近鄰演算法（ K Nearest Neighbor）
(D) 羅吉斯迴歸（ Logistic Regression）


14降維
D 14. 下列何者不是降維的好處？
(A) 減少運算時間與儲存空間
(B) 移除共線性資料能有效提高線性模型的效能
(C) 當資料維度降至 2～3 維時，能很容易的直接視覺化展示資料分佈
(D) 降維後的資料集訊息量增加，不會減少

65降維
D 65. 下列何者不是常見的資料維度降維方法？
(A) 主成分分析（ Principle Component Analysis）
(B) 核主成分分析（ Kernel PCA）
(C) 多維尺度法（ Multidimensional Scaling）
(D) K 平均法（ K-means）

232降維
C 232. 下列何者不是資料降維的方法？
(A) Principal Component Analysis
(B) Linear Discriminant Analysis
(C) K Nearest Neighbors
(D) Isomap

62主成分分析
A 62. 使用下列何種方法，可以重新組合資料屬性，產生新的維度？
(A) 主成分分析法（ PCA， Principle Component Analysis）
(B) K 平均法（ K-means）
(C) C50
(D) 卡方檢定（ Chi-square test）

31 PCA
D 31. 關於主成分分析（ Principal Component Analysis, PCA） 屬性萃取的主要用途，下列敘述何者正確？
(A) 以長條圖視覺化多變量資料
(B) 將低度相關的預測變數矩陣 x，轉換成相關且量多的潛在變項集合
(C) 將最攸關的訊息與無關的雜訊結合
(D) 將問題領域中的數個變數，組合成單一或數個具訊息力的特徵變數


C 4. 用主成分分析（ principle component analysis） 來處理多維度資料時，會利用相關矩陣（ correlation matrix） 來計算特徵值（ eigenvalue） 與特徵向量（ eigenvector），如果特徵向量 λ= [4.32, 1.07, 0.49, 0.10, 0.01, 0.01]，下列敘述何者正確？
(A) 主特徵值的貢獻率達到 80%
(B) 主特徵值的貢獻率達到 90%
(C) 前兩個特徵值的貢獻率達到 90%
(D) 前兩個特徵值的代表性不足

32SVD
B 32. 關於奇異值分解（ Singular Value Decomposition, SVD），下列敘述何者「不正確」？
(A) 常見應用是維度縮減
(B) 用於估計結果穩定時
(C) 用於資料其屬性個數大於觀測值個數
(D) 用於估計結果不穩定時
