---
layout      : post
title       :  分類模型
date        : 2020-04-15 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,分類模型,羅吉斯迴歸]
---

# 分類模型
假設預測變數矩陣為 $X_{n \times m}$，線性分類模型的共通想法是找到一個或多個 $x_j, j=1,2, \cdots, m$ 所形成的線性函數( i.e. $m$ 維空間的超平面 )將 $n$ 樣本投影到該平面後，盡可能正確地切分成 $k$ 類，各類分別有 $n_1, n_2, \cdots, n_k$個樣本，其中 $n_1+n_2+ \cdots +n_k=n$。

## 羅吉斯迴歸分類
羅吉斯迴歸分類，是建立二元類別機率之成功勝率(odds ratio)對數值的線性分類模型。

羅吉斯迴歸分類模型假設二元反應變數 $y$ 為二項式隨機變數：

$$ y= \begin{cases}1, \text{if the outcome is Success(S) with probability } p. \\
            0,\text{if the outcome is Failure(F) with probability }1-p.\end{cases}
$$
這裡不是直接對 $y$ 建模，而是建立勝率比的線性模型。

先定義觀察到 $\textbf{x} = (x_1, x_2, \cdots] x_m)$ 後事件$S$發生的機率為 $p$ ，亦即 $p=Pr[y=1\mid \textbf{x}]$ ，事件$S$不發生的機率為$1-p=Pr[y=0 \mid \textbf{x}]$。透過勝率比(odds ratio)來建立下面模型：
$$ log(\frac{p}{1-p}) = \hat f(\textbf{x}) = b_0+b_1x_1+ \cdots + b_mx_m=z$$
上式經過轉換與移項整理後得到下式
$$ p = \frac{1}{1+e^{-z}}$$
此式子為線性預測子 $z$ 的 S型函數(Sigmoid function)，又稱為羅吉斯函數(logistic function)。它將傳入的線性組合值限縮在0和1之間，以估計事件發生之機率。

最後再設定分類門檻值將機率轉換成分類類別。

## 進階討論
### 討論1
1. 羅吉斯迴歸分類與神經網絡的感知機(perceptron)是等價的。

2. 若模型項沒有包含預測變數的非線性函數，則羅吉斯迴歸模型是以線性的類別邊界進行二元分類。

3. 羅吉斯回歸分類與OLS的多元線性回歸都屬於廣義線性模型(Generalized Linear Models, GLM)中的技術，GLM還包含許多反應變數 $y$ 的不同機率分佈模型。值得注意的是即使與 $p$ 的關係是非線性的，但GLM仍以線性分類邊界完成分類工作。

### 討論2
1. 分類模型建模的重點是反應變數 $y$ 在給定 $\textbf x$ 的條件機率 $Pr[y=l \mid \textbf{x}]$。

2. 羅吉斯迴歸是直接對此後驗機率 $Pr[y=l \mid \textbf{x}]$ 建模；

3. 線性判別分析之貝式法，並不是直接處理後驗機率，反過來先估計先驗機率 $Pr[y=l \mid \textbf{x} ]$ 與 $Pr[y=l]$ 再取得後驗機率的訊息。

4. 羅吉斯迴歸分類適合二元分類問題，線性判別分析常用於多類樣本分類。此外，當兩類樣本分散良好，或 $\textbf{x}$ 近似常態且小樣本時，線性判別分析的估計較為穩定。所以兩種方法沒有孰優孰劣的結論，而是應該注意其適用時機。
