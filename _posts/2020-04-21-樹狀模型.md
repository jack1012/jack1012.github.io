---
layout      : post
title       :  樹狀模型
date        : 2020-04-21 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,樹狀模型]
---

# 樹狀模型
樹狀模型可以分類為預測類別的分類樹(classification trees)，與預測數值變數的迴歸樹(regression trees)。兩者建模邏輯大體相同，都是以遞分割的程序，根據預測變數與反應變數的共同分佈，持續將大小為 $n$ 的訓練樣本切分為同質群組，使得每群組反應變數的數值分佈較為簡單，或是類別變數異質程度達到最小(i.e.純度衡量達到最大)，再對各子群進行預測建模。

樹狀模型的建構過程牽涉下面三個重要的決定：
1. 分割資料集的預測變數與其分割值；
2. 樹的深度或複雜度；
3. 葉節點的預測方程或方法。

一般而言一棵“完全生長”的決策樹包含，特徵選擇、決策樹構建、剪枝三個過程。剪枝再另外討論。

首先分類樹會依據各預測變數及其可能的分割值，衡量目標類別變數分割前後的異質程度改善幅度。在ID3中選擇熵減少程度最大的特徵來劃分資料，也就是“最大資訊熵增益”原則，而CART是以吉尼不純度為衡量標準。

### 樹狀模型優缺點
缺點：
- 模型不穩定，訓練資料中有些許變動，可能導致不同的樹狀模型決策邏輯
- 算法參數如未適當調校，容易過度配適，或產生配適不足
- 過大的樹其結構難以解釋，且容易造成決策違反直覺的狀況，所以當問題不適合此種知識表達方式時，樹狀模型的績效就不會是最佳的。
- 樹建構算法要避免偏誤，留意是否傾向於挑選值特別多的屬性


## 類別變數的分散程度指標：
其中類別變數共有 $k$ 個不同類別值，而 $p_i$ 是第 $i$ 個類別的比例。用以衡量資料樣本集合於分類別上資料的分散程度，以下介紹兩類常用指標：
### 熵係數 (entropy cofficent)
$$ E = -\sum_{i=1}^{k}p_i \log p_i$$

### 吉尼不純度(Gini impurity)
$$ G= 1 - \sum_{i=1}^{k}p_i^2 $$

- 完美同質：集中於某一類
    - 熵係數為0、吉尼不純度為0
- 完美異質：各類別分佈平均，係數最大值
    - 熵係數為 $\log k$、吉尼不純度為 $1-\frac{1}{k}$

常見的樹狀模型演算法有ID3、C4.5、CART。

## ID3(Iterative Dichotmiser 3)
ID3算法是使用熵係數 (entropy cofficent)來計算訊息增益(information gain)：

對於樣本集合 $D$，類別數為 $K$，資料 $D$ 的熵值為
$$H(D) = - \sum_{k=1}^{K}\frac{|C_k|}{|D|} \log \frac{|C_k|}{|D|} $$
其中 $C_k$ 是樣本集合 $D$ 中屬於類別第 $k$ 類的樣本子集，$|C_k|$ 表示該子集的元素個數， $|D|$ 表示樣本集合的元素個數。

某個特徵 $A$ 對於資料集 $D$ 的條件熵值 $H(D|A)$，其中特徵 $A$ 的類別數為 $n$
$$ H(D|A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|}H(D_i)= \sum_{i=1}^{n} \frac{|D_i|} {|D|}\left( - \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|}\log \frac{|D_{ik}|}{|D_i|} \right) $$
其中 $D_i$表示 $D$ 中特徵 $A$ 第 $i$ 個值的樣本子集， $D_{ik}$ 表示 $D_i$ 中屬於第 $k$ 類的樣本子集。

PS：若特徵 $A$ 與 資料集 $D$的類別一樣時， $H(D|A) =0$

訊息增益為
$$g(D,A) = H(D) - H(D|A)$$
 以衡量以特徵 $A$ 分割後得到多少訊息，亦即熵值降低了多少。不失一般性假設最大訊息增益 $g(D,A)$ 為特徵 $A$，即以特徵 $A$ 進行分割，作為樹的節點。

PS：$H(D|A)$ 又稱為分割後的平均訊息量。


## C4.5
ID3採用的資訊增益度量存在一個缺點，它一般會優先選擇有較多屬性值的特徵(Feature)，因為屬性值多的特徵會有相對較大的資訊增益(資訊增益反映的給定一個條件以後不確定性減少的程度，必然是分得越細的資料集確定性更高，也就是條件熵越小，資訊增益越大)。為了避免這個不足C4.5中是用資訊增益比率(gain ratio)來作為選擇分支的準則。

C4.5算法是使用資訊增益比(information gain ratio)為
$$g_{R}(D,A) = \frac{g(D,A)}{H_{A}(D)}$$
其中
$${H_{A}(D)}= - \sum_{i=1}^{n}\frac{|D_i|}{|D|}\log \frac{|D_i|}{|D|}$$
$n$為特徵 $A$ 的類別數，$D_i$ 是樣本集合 $D$ 中屬於 特徵 $A$的第 $i$ 類的樣本子集，$|D_i|$ 表示該子集的元素個數， $|D|$ 表示樣本集合的元素個數。

稱為資料集 $D$關於特徵 $A$ 的取熵值，或稱為本質訊息。

### 分類與迴歸樹(Classification and Regression Trees, CART)
CART算法是使用吉尼不純度(Gini impurity)來計算訊息增益：

對於樣本集合 $D$，類別數為 $K$，資料 $D$ 的Gini值為
$$\operatorname*{Gini}(D)=1-\sum_{k=1}^{K}\left( \frac{|C_k|}{|D|}\right)^2$$

其中 $C_k$ 是樣本集合 $D$ 中屬於類別第 $k$ 類的樣本子集，$|C_k|$ 表示該子集的元素個數， $|D|$ 表示樣本集合的元素個數。

某個特徵 $A$ 對於資料集 $D$ 的條件Gini值為 $\operatorname*{Gini}(D|A)$，其中特徵 $A$ 會被成兩類
$$ \operatorname*{Gini}(D|A) =  \frac{|D_1|}{|D|}\operatorname*{Gini}(D_1) + \frac{|D_2|}{|D|}\operatorname*{Gini}(D_2)$$
其中 $D_i$表示 $D$ 中特徵 $A$ 第 $i$ 個值的樣本子集， $D_{ik}$ 表示 $D_i$ 中屬於第 $k$ 類的樣本子集。

Gini增益為
$$\operatorname*{GiniGain}(D,A) = \operatorname*{Gini}(D) - \operatorname*{Gini}(D|A)$$
CART在每一次的迭代過程中選擇Gini值最小的特徵及其相對應的切分點進行分類。但與ID3、C4.5不同的是，CART是一顆二元樹，採用二元切割法，每一步將資料按特徵 $A$ 的取值切分成兩份，分別進入左右子樹。


## 進階討論
1. ID3算法可以歸納為以下幾點：
- 使用屬性計算與之相關的樣本熵值
- 選取其中熵值最小的屬性(資訊獲利最大)
- 生成包含該屬性的節點
- 遞迴直到終止

2. C4.5的改善：
- 對連續屬性的處理
- 改善ID3傾向選擇擁有許多不同數值但不具意義的屬性：之所以使用獲利比率(Gain Ratio)，是因為ID3演算法所使用的資訊獲利會傾向選擇擁有許多不同數值的屬性，例如：若依學生學號(獨一無二的屬性)進行分割，會產生出許多分支，且每一個分支都是很單一的結果，其資訊獲利會最大。但這個屬性對於建立決策樹是沒有意義的。

- C5.0 是 C4.5的商業改進版，可應用於海量資料集合上之分類。主要在執行準確度和記憶體耗用方面做了改進。因其採用Boosting方式來提高模型準確率，且佔用系統資源與記憶體較少，所以計算速度較快。其所使用的演算法沒有被公開。

- C5.0 的優點：
  - C5.0模型在面對遺漏值時非常穩定。
  - C5.0模型不需要很長的訓練次數。
  - C5.0模型比較其他類型的模型易於理解。
  - C5.0的增強技術提高分類的精度。

3. 屬性選擇指標的其他度量標準
- 訊息獲利 : 趨向於包含多個值的屬性
- 獲利比率 : 會產生不平均的分割，也就是分割的一邊會非常小於另一邊
- 吉尼係數 : 傾向於包含多個值的屬性，當類別個數很多時會有困難，傾向那些會導致平衡切割並且兩邊均為純粹的測試

4.  ID3只能處理離散型變數，而C4.5和CART都可以處理連續型變數。C4.5處理連續型變數時，透過對資料排序之後找到類別不同的分割線作為切分點，根據切分點把連續屬性轉換為布林型，進而將連續型變數轉換多個取值區間的離散型變數。而對CART，由於其建構時每次都會對特徵進行二值劃分，因此可以充分適用於連續型變數。

5.  ID3對於樣本特徵缺失值比較敏感，而C4.5和CART都可以對缺失值進行不同的處理方式。

6.  ID3和C4.5可以以每個節點上產出多叉分支，且每個特徵在層級之間不會重用，而CART每個節點只會產生兩個分支，因此最後形成一棵二元樹，且每個特徵可以重複使用。

7.  ID3與C4.5透過剪枝來權衡樹的精準性，與泛化能力，而CART直接利用全部資料發現所有可能的樹結構進行比對。

8.  關於決策樹（ decision tree）的使用， 下列敘述何者「不」正確？(C\)
    - (A) 每次分割要讓子節點不純度降低
    - (B) 樹葉節點中的樣本應屬於同一類別
    - (C\) 為了進行準確的預測，要讓決策樹盡量長大長深
    - (D) 結合多個決策樹形成隨機森林（ random forest），模型較穩健，不易過度配適

9.  一般說來， 拔靴集成法（ Bootsrap AGGregatING, BAGGING）其集成模型中各株決策樹（ decision tree） 是__________；效能提升法（boosting）其集成模型中各株樹則是__________。 (D)
    - (A) 經過修剪的（ pruned）；經過修剪的（ pruned）
    - (B) 經過修剪的（ pruned）；未經修剪的（ unpruned）
    - (C) 未經修剪的（ unpruned）；未經修剪的（ unpruned）
    - (D) 未經修剪的（ unpruned）；經過修剪的（ pruned）

10. 關於決策樹（ decision tree） 與隨機森林（ random forest） 的比較， 下列敘述何者正確？ (D)
    - (A) 決策樹屬於監督式學習（ supervised learning），隨機森林屬於非監督式學習（ unsupervised learning）
    - (B) 兩者皆屬於薈萃式學習（ ensemble learning）
    - (C) 隨機森林的每一棵決策樹之間是有關聯的
    - (D) 隨機森林能處理離散型資料，也能處理連續型資料

11. R 語言的 rpart 套件，實現了_________算法的諸多概念？ (C\)
    - (A) 迭代二分樹第三代（ Iterative Dichotomiser 3, ID3）算法
    - (B) C4.5
    - (C\) 分類與迴歸樹（ Classification and Regression Trees, CART）
    - (D) C5.0
