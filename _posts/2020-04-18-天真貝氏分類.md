---
layout      : post
title       :  天真貝氏分類
date        : 2020-04-18 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,分類模型,天真貝氏分類]
---

# 天真貝氏分類 (單純貝氏分類)
天真貝氏（Naïve-Bayes）分類法是以機率或概似估計新案例屬於不同類的可能性(likelihood)，它是基於貝氏定理發展出來的分類方法。無適當假設時天真貝氏的計算比較耗時，但在屬性間滿足條件獨立性的天真假設下，能處理非常大量的資料集，常用於文本分類，例如：郵件與簡訊分類。

## 貝氏定理
貝氏定理是關於隨機事件A和B的條件概率的一則定理。

$$P(A\mid B)={\frac {P(B\mid A)P(A)}{P(B)}}$$

其中 $A$ 以及 $B$ 為隨機事件，且 $P(B)$ 不為零。$P(A \mid B)$ 是指在事件$B$發生的情況下事件$A$發生的概率。

在貝氏定理中，每個名詞都有約定俗成的名稱：
- $P(A \mid B)$ 是已知 $B$ 發生後，$A$ 的條件概率。也由於得自 $B$ 的取值而被稱作 $A$ 的後驗概率。
- $P(A)$ 是 $A$ 的先驗概率（或邊緣概率）。之所以稱為"先驗"是因為它不考慮任何 $B$ 方面的因素。
- $P(B \mid A)$ 是已知 $A$ 發生後，$B$ 的條件概率。也由於得自 $A$ 的取值而被稱作 $B$ 的後驗概率。
- $P(B)$ 是 $B$ 的先驗概率。

按這些術語，貝氏定理可表述為：
後驗概率 = (似然性*先驗概率)/標准化常量
也就是說，後驗概率與先驗概率和相似度的乘積成正比。

另外，比例$P(B \mid A)/P(B)$ 也有時被稱作標准似然度（standardised likelihood），貝氏定理可表述為：
後驗概率 = 標准似然度*先驗概率

## 天真貝氏模型假設
天真貝氏模型直接假設所有的隨機變數之間具有條件獨立的情況，因此可以直接利用條件機率相乘的方法，計算出聯合機率分佈：
$$p(X \mid C)=P(X_1 \mid C)P(X_2 \mid C) \cdots P(X_d \mid C)$$
其中 $X=[X_1,X_2, \cdots ,X_d]$ 是一個特徵向量，而 $C$ 代表一個特定類別。

## 天真貝氏模型的推導
理論上，機率模型分類器是一個條件機率模型。
$$p(C\vert F_{1},\dots ,F_{n})$$
獨立的類別變數 $C$ 有若干類別，條件依賴於若干特徵變數 $F_1, \cdots ,F_n$。但問題在於如果特徵數量 $n$ 較大或者每個特徵能取大量值時，基於機率模型列出機率表變得不現實。所以我們修改這個模型使之變得可行。 貝葉斯定理有以下式子：

$$p(C\vert F_1,\cdots ,F_n)={\frac {p(C)\ p(F_1,\cdots ,F_n\vert C)}{p(F_1,\cdots ,F_n)}}$$

推倒過程參考 <https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8>

$$p(C\vert F_{1},\dots ,F_{n})={\frac  {1}{Z}}p(C)\prod _{{i=1}}^{n}p(F_{i}\vert C)$$
其中 $Z$ (證據因子)是一個只依賴與 $F_1, \cdots ,F_{n}$ 等的縮放因子，當特徵變數的值已知時是一個常數。 由於分解成所謂的類先驗機率 $p(C)$ 和獨立機率分佈 $p(F_{i}\vert C)$，上述機率模型的可掌控性得到很大的提高。如果這是一個 $k$ 分類問題，且每個 $p(F_{i}\vert C=c)$ 可以表達為 $r$ 個參數，於是相應的單純貝氏模型有 $(k − 1)+ n r k$ 個參數。實際應用中，通常取 $k=2$（二分類問題），$r=1$ (伯努利分佈作為特徵，有或沒有），因此模型的參數個數為 $2n+1$，其中 $n$ 是二值分類特徵的個數。

## 從機率模型中構造分類器
討論至此為止我們匯出了獨立分佈特徵模型，也就是單純貝氏機率模型。單純貝氏分類器包括了這種模型和相應的決策規則。一個普通的規則就是選出最有可能的那個：這就是大家熟知的最大後驗機率（MAP）決策準則。相應的分類器便是如下定義的 $\mathrm {classify}$ 公式：

$$\mathrm {classify} (f_{1},\cdots ,f_{n})={\underset {c}{\operatorname {argmax} }}\ p(C=c)\displaystyle \prod _{i=1}^{n}p(F_{i}=f_{i}\vert C=c)$$

## 參數估計
所有的模型參數都可以通過訓練集的相關頻率來估計。常用方法是機率的最大似然估計。類的先驗機率可以通過假設各類等機率來計算（先驗機率 = 1 / (類的數量)），或者通過訓練集的各類樣本出現的次數來估計（A類先驗機率=（A類樣本的數量）/(樣本總數)）。為了估計特徵的分佈參數，我們要先假設訓練集資料滿足某種分佈或者非參數模型。

### 高斯單純貝氏
如果要處理的是連續資料一種通常的假設是這些連續數值為高斯分佈。 例如，假設訓練集中有一個連續屬性，$x$。我們首先對資料根據類別分類，然後計算每個類別中 $x$ 的均值和變異數。令 $\mu _{c}$  表示為 $x$ 在 $c$ 類上的均值，令 $\sigma _{c}^{2}$ 為 $x$ 在 $c$ 類上的變異數。在給定類中某個值的機率，$P(x=v \mid c)$，可以通過將 $v$ 表示為均值為 $\mu _{c}$ 變異數為 $\sigma _{c}^{2}$ 常態分佈計算出來。如下：
$$P(x=v \mid c)={\tfrac {1}{\sqrt {2\pi \sigma _{c}^{2}}}}\,e^{-{\frac {(v-\mu _{c})^{2}}{2\sigma _{c}^{2}}}}$$

處理連續數值問題的另一種常用的技術是通過離散化連續數值的方法。通常，當訓練樣本數量較少或者是精確的分佈已知時，通過機率分佈的方法是一種更好的選擇。在大量樣本的情形下離散化的方法表現更優，因為大量的樣本可以學習到資料的分佈。由於單純貝氏是一種典型的用到大量樣本的方法（越大計算量的模型可以產生越高的分類精確度），所以單純貝氏方法都用到離散化方法，而不是機率分佈估計的方法。

- K近鄰法分類
- 支援向量機分類
- 分類與回歸樹

## 天真貝式分類

## 進階討論
1. 下列何種方法，最適合解決非線性分類問題及高維空間數據識別問題？ A
- (A) 支援向量機（Support Vector Machine）
- (B) K平均法（K-means）
- (C) 貝式判別（Bayesian Classifier）
- (D) 模糊理論（Fuzzy Set）

2. 天真貝氏算法的優缺點如下：
- 優點
  - 簡單、快速且有效
  - 可以處理帶雜訊與有遺缺值得資料
  - 容易獲得類別機率值的預測值
- 缺點
  - 屬性同等重要且互相獨立的假設通常不符合現實狀況。
  - 不適合有大量數值屬性的資料集
  - 類別機率值須轉為類別標籤預測值方能做出具體決策
