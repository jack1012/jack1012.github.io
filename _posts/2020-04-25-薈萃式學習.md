---
layout      : post
title       :  薈萃式學習
date        : 2020-04-22 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,薈萃式學習]
---

# 薈萃式學習(集成學習)

集成學習的兩個步驟：
1、先構建： 如何得到若干個個體學習器、弱學習器、基礎學習器、基學習器
同質的 異質的
2、後結合： 如何選擇一種結合策略，將這些個體學習器集合成一個強學習器

迴歸
- Bagging ：平均法、帶權平均法、學習法
- Boosting：直接疊加、正則後疊加、學習法（Stacking）

分類
- Bagging ：投票法、帶權投票法、學習法
- Boosting ：直接疊加、正則後疊加、學習法

參考網址：<https://www.twblogs.net/a/5d578c60bd9eee541c31d20d>
進階閱讀：<https://read01.com/zh-tw/aA27eB6.html#.Xz9ozMgzbIU>


## 拔靴集成法(Bootstrap AGGregatING,  BAGGING)

- 用於集成不穩定的學習模型效果很好，例如樹狀模型。
- 不穩定的學習模型可以確保集成模型的多樣性。
- 袋外樣本式估計模型效能的最佳子集(1/e 約0.38)。


## 效能提升法(boosting) 多模激發法
- XGBoost、LightGBM兩個比較新的算法
- XGBoost無法像LightGBM直接處理類別屬性，與隨機森林依樣XGBoost只能接受數值資料


## 隨機森林(random forest)
隨機森林
bagging方法可以有效降低模型的方差。隨機森林每棵子樹不需要剪枝，是低偏差高方差的模型，通過bagging降低方差後使得整個模型有較高的性能。

隨機森林其實很簡單，就是在bagging策略上略微改動了一下。

1. 從N個樣本中有放回的隨機抽樣n個樣本。
2. 如果每個樣本的特徵維度為M，指定一個常數m<< M，隨機地從M個特徵中選取m個特徵子集，每次樹（ID3,C4.5,CART）進行分裂時，從這m個特徵中選擇最優的(信息增益，信息增益比率，基尼係數)；
3. 每棵樹都盡最大程度的生長，並且沒有剪枝過程。
4. 最後採用投票表決的方式進行分類。

隨機森林有許多優點：

- 具有極高的準確率
- 隨機性的引入，使得隨機森林不容易過擬合
- 隨機性的引入，使得隨機森林有很好的抗噪聲能力
- 能處理很高維度的數據，並且不用做特徵選擇
- 既能處理離散型數據，也能處理連續型數據，數據集無需規範化
- 訓練速度快，可以得到變量重要性排序
- 容易實現並行化

隨機森林的缺點：
- 當隨機森林中的決策樹個數很多時，訓練時需要的空間和時間會較大
- 隨機森林模型還有許多不好解釋的地方，有點算個黑盒模型


## 進階討論

- BAGGING解決模型變異過高的手段，添加建模過程的隨機噪訊，反而能有效降低變異
- Boosting是逐漸提高訓練難度與變換不同方向，避免建構重蹈覆轍的模型
- RandomForest除了降低變異(Bagging)去除模型中基本模型之間的相關性(Boosting)，雙向改善
