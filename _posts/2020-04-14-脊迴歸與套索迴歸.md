---
layout      : post
title       :  脊迴歸與套索迴歸
date        : 2020-04-14 10:41:10 +0800
categories  : 進階機器學習
tags        : [機器學習,迴歸模型,脊迴歸,套索迴歸]
---

普通最小平方法(Ordinary Least Squares, OLS)是最小化誤差平方和(Sum Square Error, SSE)，當模型過度配適訓練資料時，或有共線性、 $m>n$ 時，OLS的係數估計值會膨脹。因此，我們這時候要透過估計(或參數優化)過程控制係數估計值的大小，以降低SSE，這種過程稱為係數正規化(regularization)。在OLS的目標函數SSE加上由各個迴歸係數所產生的懲罰項目(penalized iterm)。其中脊迴歸(Ridge regression)是加上 $m$ 個迴歸係數的平方和，也就是迴歸係數與原點之 $L_2$ 範數的平方

另外一種稱為套索迴歸(Least Absolute Shrinkage and Selection Operator, LASSO)是對SSE加上 $m$ 個迴歸係數的絕對值和，也就是迴歸係數與原點之 $L_1$ 範數。

雖然兩種方法只有小小的不同，但實際的意義卻非常重要。雖然迴歸係數會朝向零的方向縮減，但絕對值得懲罰項會使得某些迴歸係數在是當的 $\lambda$ 值下精確地降到零。也就是說，脊迴歸傾向將迴歸係數值平均地分散在相關的預測變數之間，而LASSO迴歸則是挑出最重要的一個，並忽略剩下的相關預測變數。

## 脊迴歸(Ridge Regression)
$$ SSE_{L_{2}} = \sum_{i=1}^n (y_i-\hat y_i)^2+\lambda \sum_{j=1}^n\beta^2_j $$


##  套索迴歸(LASSO Regression)
$$ SSE_{L_{1}} = \sum_{i=1}^n (y_i-\hat y_i)^2+\lambda \sum_{j=1}^n \left| \beta_j \right|$$


## 進階討論
懲罰項的效果是如果該迴歸係數可讓SSE明顯地降低，則其係數估計值被容許放大。因此，當懲罰係數 $\lambda$ 逐漸增大時，此法會將相對不重要的變數估計值依序縮減到零，所以這種配適方法又被稱為係數縮減法(shrinkage methods)。
